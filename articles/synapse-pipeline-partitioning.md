# Parameterized Partitioning and Copy Process Optimization

# Overview
This documentation outlines the steps required to parameterize partitions in your copy process, enhancing throughput and maintaining the flexibility of your existing framework. The approach leverages configuration table updates and best practices in partition parameterization and parallelism.

# Updating the Configuration Table
To enable parameterized partitioning, update your configuration table by adding the necessary columns for partition boundaries. This allows explicit definition of partition ranges, avoiding the limitations and potential delays of automatic partitioning.
# Partition Boundary Recommendations
- **Lower Bound:** Set this value to represent the lowest 20% of overall records.
- **Upper Bound:** Set this value to represent the highest 80% of overall records.
Explicitly specifying these boundaries ensures greater control over data selection and optimizes partitioning efficiency.
# Parallelism and Max Rows Per File
When setting up parallel processing (for example, a parallelism degree of 4), evenly divide the "Max Rows Per File" value across each thread. This ensures balanced workload distribution and optimal performance during the copy process.
# Manual Formulas
All relevant formulas for partitioning and parallelism should be entered manually as described above to maintain precision and consistency.
# Potential benign Errors and Notes
- There may be a validation error displayed in the sink section for the "Max Rows Per File" parameterized value.
- Functionality remains intact despite this message; the parameterization should work as intended.
# Additional Notes
- Always check the copy activity input file to confirm configuration.
- Ensure that no errors occur when running the pipeline.
- When making modifications, copy the current pipeline and dataset first to preserve original configurations, especially since recent changes may affect metadata specifications.

![image](https://github.com/user-attachments/assets/9cdfcf3d-8d55-4bb7-929f-933aca795672)

Lastly and most importantly, the partition column name should be a column with unique values, therefore, I chose the primary key value as you can see below in step 3. 

Below are the overall steps: 


# Step 1: 
Alter the configuration table to add the four below columns to leverage dynamic partition for the loop: 

```
alter table [POC].[adf].[ConfigTable_DW_Synapse] add  Partition_Column_Name varchar(250);
alter table [POC].[adf].[ConfigTable_DW_Synapse] add  Partition_Lower_Bound bigint;
alter table [POC].[adf].[ConfigTable_DW_Synapse] add  Partition_Upper_Bound bigint;
alter table [POC].[adf].[ConfigTable_DW_Synapse] add  MaxRowsPerFile bigint;
```


# Step 2: 
You will have to modify the source query to the below query. Please see the export template for more information if required. 
You will notice “?AdfDynamicRangePartitionCondition AND” which is how the Data Factory service will process the partitions dynamically. 
Please note to save time and space I only exported two activities in the pipeline that are responsible for the partition process. 

```
select @{item().SourceColumns} from @{item().SourceTable} where  ?AdfDynamicRangePartitionCondition AND @{item().WatermarkColumn} > ('@{item().WatermarkValue}') AND 
@{item().WatermarkColumn} <= ('@{activity('Lookup1_copy1').output.firstrow.MAXVALUE}')      
```

![image](https://github.com/user-attachments/assets/7dec2552-7994-448b-a664-b8f0dd211663)


# Step 3: 
Use the below formula for each table in the configuration table to get the values and add it to the above columns: 
While this is a manual process, this should theoretically not change much and doing it manually will save time so that the partition process will not have to select the overall records during execution and we will not hit the limitation as well.
Examples: 

```
select count(*)*.20 from Microsoft_POC.Adven  -- lower bound value 
select count(*)*.80 from Microsoft_POC.Sales   --upper bound value 
select count(*)/4 from Microsoft_POC.Products      --max rows per file 
```

![image](https://github.com/user-attachments/assets/c50938e4-5a1c-4a10-a762-512497b1a6f1)


# Step 4: 
Please review the Synapse_POC_Partition_Update.zip file in the secure file transfer for the changes required for the pipeline, the illustration is below. 
Please also ensure that you create a new dataset by copying the old one and specifying it for the sink as the new data set, to use partitions and multiple files, we no longer can specify a file name. 
The file name will be auto generated by the service.  
If a new dataset is not created before making the below changes, you will break all of your other pipelines that are dependent on it. 
Also notice that now we have to add the table and schema name below since ADF will auto generate the file name from those values.

![image](https://github.com/user-attachments/assets/8089dd51-04fa-448e-99b6-74885760c163)


![image](https://github.com/user-attachments/assets/5e31cf49-c95f-4132-b63e-8f4ef98779a8)


![image](https://github.com/user-attachments/assets/d7975caa-41d5-46e6-9bc9-c43c9129a6cc)

![image](https://github.com/user-attachments/assets/95a0f11f-dd68-40b1-a7b5-d692293424ae)



# Step 5: 
Lastly, in the copy activity from on-prem SQL to Azure Data Lake  the settings tab, you will need to modify the “Degree of copy parallelism” to 4 as the below illustration: 
This will allow us to use parallel threads when processing the work. 


![image](https://github.com/user-attachments/assets/6ee1e5cc-eaa0-4912-bf8e-68505113680a)







***DISCLAIMER: Sample Code is provided for the purpose of illustration only and is not intended to be used in a production environment unless thorough testing has been conducted by the app and database teams. 
THIS SAMPLE CODE AND ANY RELATED INFORMATION ARE PROVIDED "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF MERCHANTABILITY AND/OR FITNESS 
FOR A PARTICULAR PURPOSE. We grant You a nonexclusive, royalty-free right to use and modify the Sample Code and to reproduce and distribute the object code form of the Sample Code, provided that. You agree: (i) 
to not use Our name, logo, or trademarks to market Your software product in which the Sample Code is embedded; (ii) to include a valid copyright notice on Your software product in which the Sample Code is 
embedded; and (iii) to indemnify, hold harmless, and defend Us and Our suppliers from and against any claims or lawsuits, including attorneys fees, that arise or result from the use or distribution or use of the 
Sample Code.***
